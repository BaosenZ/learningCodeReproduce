








# import packages
import tensorflow as tf
from tensorflow import keras
import numpy as np
import matplotlib
import matplotlib.pyplot as plt
import pandas as pd

print("tf version: ", tf.__version__)
print("keras version: ", keras.__version__)
print("np version: ", np.__version__)
print("matplotlib version: ", matplotlib.__version__)
print("pd version: ", pd.__version__)


# Read the excel file

dataset_path = "../au2c00122_si_002.xlsx"
train_dataset = pd.read_excel(dataset_path, sheet_name='training set')
val_dataset = pd.read_excel(dataset_path, sheet_name='validation set')

val_dataset.head()





# Split x and y
x_train_df = train_dataset.iloc[:, 0:2291]
y_train_df = train_dataset.iloc[:, 2291]
x_val_df = val_dataset.iloc[:, 0:2291]
y_val_df = val_dataset.iloc[:, 2291]

# Convert df to numpy array
x_train = x_train_df.to_numpy()
y_train = y_train_df.to_numpy()
x_val = x_val_df.to_numpy()
y_val = y_val_df.to_numpy()

print(x_train[0])
print(y_train[0])
print(x_train.shape)





# Build the model (sequential)
model = keras.models.Sequential()
model.add(keras.layers.Input([2291]))
model.add(keras.layers.Dense(512, kernel_initializer='normal', kernel_regularizer='l2'))
model.add(keras.layers.PReLU())
model.add(keras.layers.Dropout(rate=0.5))
model.add(keras.layers.Dense(128, kernel_initializer='normal', kernel_regularizer='l2'))
model.add(keras.layers.PReLU())
model.add(keras.layers.Dropout(rate=0.5))
model.add(keras.layers.Dense(16, kernel_initializer='normal', kernel_regularizer='l2'))
model.add(keras.layers.PReLU())
model.add(keras.layers.Dropout(rate=0.5))
model.add(keras.layers.Dense(1, kernel_initializer='normal'))

# Compile the model
optimizer = keras.optimizers.ADAM(learning_rate=0.00001)
model.compile(loss="mean_absolute_error", optimizer=optimizer, metrics=[keras.metrics.RootMeanSquaredError(), keras.metrics.R2Score()])

model.summary()


# Train the model
history = model.fit(x_train, y_train, epochs=15000, validation_data=[x_val, y_val])


# Save the model
model_name = "trained_model.keras"
model.save(model_name)





# build the model and define the search space using argument hp

def build_model(hp):
    model = keras.models.Sequential()

    # Input layer
    model.add(keras.layers.Input([2291]))
    
    # Define activation choice for all hidden layer
    activation_choice=hp.Choice("activation_all_layer", ["prelu", "relu", "tanh", "softmax"])

    # 1st layer
    if activation_choice == "prelu":
        model.add(keras.layers.Dense(
            # Tune number of units. 
            units=hp.Int("units", min_value=384, max_value=640, step=128),
            kernel_initializer='normal',
            # Set l2 weigh decay as 0.01
            kernel_regularizer=keras.regularizers.l2(0.01)
        ))
        model.add(keras.layers.PReLU())  # Add PReLU as a separate layer
    else:
        model.add(keras.layers.Dense(
            units=hp.Int("units", min_value=384, max_value=640, step=128),
            activation=activation_choice,
            kernel_initializer='normal',
            kernel_regularizer=keras.regularizers.l2(0.01)
        ))
    # Tune dropout rate.
    dropout_choise=hp.Choice("dropout_rate", [0.25, 0.5, 0.75])
    model.add(keras.layers.Dropout(rate=dropout_choise))
    
    # 2nd layer
    if activation_choice == "prelu":
        model.add(keras.layers.Dense(
            # Tune number of units. 
            128, 
            kernel_initializer='normal',
            # Set l2 weigh decay as 0.01
            kernel_regularizer=keras.regularizers.l2(0.01)
        ))
        model.add(keras.layers.PReLU())  # Add PReLU as a separate layer
    else:
        model.add(keras.layers.Dense(
            128, 
            activation=activation_choice,
            kernel_initializer='normal',
            kernel_regularizer=keras.regularizers.l2(0.01)
        ))
    # Tune dropout rate.
    model.add(keras.layers.Dropout(rate=dropout_choise))
    
    # 3rd layer
    if activation_choice == "prelu":
        model.add(keras.layers.Dense(
            # Tune number of units. 
            16,
            kernel_initializer='normal',
            # Set l2 weigh decay as 0.01
            kernel_regularizer=keras.regularizers.l2(0.01)
        ))
        model.add(keras.layers.PReLU())  # Add PReLU as a separate layer
    else:
        model.add(keras.layers.Dense(
            16,
            activation=activation_choice,
            kernel_initializer='normal',
            kernel_regularizer=keras.regularizers.l2(0.01)
        ))
    # Tune dropout rate.
    model.add(keras.layers.Dropout(rate=dropout_choise))
    
    # output layer
    model.add(keras.layers.Dense(1, kernel_initializer='normal'))
    
    # Compile the model
    # Define the optimizer choice and learning rate choise
    optimizer_choice = hp.Choice("optimizer", ["adam", "sgd", "rmsprop"])
    # lr_choice=hp.Choice("lr", values=[1e-6, 1e-5, 1e-4])
    if optimizer_choice == "adam":
        optimizer = keras.optimizers.Adam(
            learning_rate= 1e-5
        )
    elif optimizer_choice == "sgd":
        optimizer = keras.optimizers.SGD(
            learning_rate=1e-5
        )
    elif optimizer_choice == "rmsprop":
        optimizer = keras.optimizers.RMSprop(
            learning_rate=1e-5
        )
    
    model.compile(loss="mean_absolute_error", optimizer=optimizer, metrics=[keras.metrics.RootMeanSquaredError(), keras.metrics.R2Score()])

    return model


import keras_tuner

print("keras_tuner version:", keras_tuner.__version__)


# Init the tuner
tuner = keras_tuner.BayesianOptimization(
    hypermodel=build_model,
    objective=keras_tuner.Objective(name="val_r2_score", direction="max"),
    max_trials=5,
    executions_per_trial=1,
    overwrite=True,
    directory="models_dir",
    project_name="SepaML_Project",
)

# Print a summary of the search space
tuner.search_space_summary()


# Start the search
tuner.search(x_train, y_train, epochs=15000, validation_data=(x_val, y_val))


# Query the results

# Get the top 2 models.
models = tuner.get_best_models(num_models=2)
# Print the best model
best_model = models[0]
best_model.summary()


# Print a summary of the search results.
tuner.results_summary()





# Load the best saved model

model_name = "trained_model.keras"
loaded_model = keras.models.load_model(model_name)


# Evaluate trained model

results = model.evaluate(x_train, y_train, batch_size=128)
results = model.evaluate(x_val, y_val, batch_size=128)


# plot performance vs epoch

plt.plot(history.history['root_mean_squared_error'],'r')
plt.plot(history.history['val_root_mean_squared_error'],'b')
plt.title('model performance')
plt.ylabel('RMSE')
plt.xlabel('epoch')
plt.legend(['train', 'validation'], loc='upper left')
plt.xlim([0, 5000])
plt.ylim([0, 2])
plt.show()


# plot performance vs epoch

plt.plot(history.history['r2_score'],'r')
plt.plot(history.history['val_r2_score'],'b')
plt.title('model performance')
plt.ylabel('R Square')
plt.xlabel('epoch')
plt.legend(['train', 'validation'], loc='upper left')
plt.xlim([0, 5000])
plt.ylim([0, 1])
plt.show()





# predict using the loaded model

# y_pred_train = model.predict(X_Train_arr)
# print(y_pred_train)

# y_pred_val = model.predict(X_Validation_arr)
# print(y_pred_val)



